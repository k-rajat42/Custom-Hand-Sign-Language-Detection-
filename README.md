# ðŸ¤š Hand Sign Recognition â€” A to Z

Real-time American Sign Language (ASL) alphabet recognition using **MediaPipe** hand landmarks and a **Random Forest** classifier.  
Works out-of-the-box with the included pre-trained `model/model.p`.

---

## âœ¨ Features

| Feature | Description |
|---|---|
| ðŸŽ¥ Real-time recognition | Live webcam feed, 30+ FPS |
| ðŸ”¤ Word composer | Spell words letter-by-letter with a stability filter |
| ðŸ“Š Confidence bar | Visual confidence meter per prediction |
| ðŸ”Š Text-to-speech | Speaks confirmed letters/words (optional) |
| ðŸ“¸ Screenshot capture | Press **S** to save a snapshot |
| ðŸ“ˆ Visualizations | Class distribution, landmark plots, feature importance |
| ðŸ”„ Data augmentation | Horizontal flip doubles the training set |
| âš¡ Fast inference | < 5 ms per frame on CPU |

---

## ðŸš€ Quick Start (with pre-trained model)

```bash
# 1. Clone the repo
git clone https://github.com/YOUR_USERNAME/hand-sign-recognition.git
cd hand-sign-recognition

# 2. Install dependencies
pip install -r requirements.txt

# 3. Run inference â€” uses model/model.p directly
python inference_classifier.py
```

### Keyboard shortcuts during inference

| Key | Action |
|---|---|
| `SPACE` | Insert space |
| `BACKSPACE` | Delete last character |
| `ENTER` | Confirm current word |
| `S` | Save screenshot |
| `Q` | Quit |

---

## ðŸ“ Project Structure

```
hand-sign-recognition/
â”œâ”€â”€ collect_imgs.py          # Step 1 â€” capture images per letter
â”œâ”€â”€ create_dataset.py        # Step 2 â€” extract MediaPipe landmarks
â”œâ”€â”€ train_classifier.py      # Step 3 â€” train & evaluate classifier
â”œâ”€â”€ inference_classifier.py  # Step 4 â€” real-time recognition âœ¦ start here
â”œâ”€â”€ graph.py                 # Visualise data & model stats
â”œâ”€â”€ model/
â”‚   â”œâ”€â”€ model.p              # Pre-trained model (RandomForest + LabelEncoder)
â”‚   â””â”€â”€ data.pickle          # Landmark dataset (generated by create_dataset.py)
â”œâ”€â”€ data/                    # Raw images (A-Z folders, not tracked by git)
â”œâ”€â”€ docs/                    # Auto-generated plots
â”œâ”€â”€ screenshots/             # Saved during inference
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ðŸ›  Train Your Own Model

```bash
# Step 1 â€” Collect images (200 per letter, A-Z)
python collect_imgs.py --size 200

# Step 2 â€” Extract landmarks & build dataset
python create_dataset.py --augment

# Step 3 â€” Train (Random Forest by default)
python train_classifier.py

# Optional â€” Gradient Boosting
python train_classifier.py --model gb --trees 300

# Step 4 â€” Run!
python inference_classifier.py
```

### Advanced inference flags

```bash
python inference_classifier.py \
  --model   model/model.p \   # custom model path
  --camera  0 \               # webcam index
  --confidence 0.65 \         # min confidence to accept prediction
  --stability  20 \           # frames needed to confirm a letter
  --tts                       # enable text-to-speech
```

---

## ðŸ“Š Visualize

```bash
python graph.py
# Saves to docs/:
#   class_distribution.png
#   sample_landmarks.png
#   feature_importance.png
```

---

## ðŸ§  How It Works

1. **MediaPipe Hands** detects 21 3D landmarks per hand.
2. Landmarks are normalised (relative to wrist position) â†’ 42 features.
3. A **Random Forest** classifier (200 trees) predicts the letter.
4. A **stability buffer** requires the same letter across *N* consecutive frames before accepting it â€” eliminates flicker.

---

## ðŸ“¦ Dependencies

| Package | Purpose |
|---|---|
| `opencv-python` | Camera capture & display |
| `mediapipe` | Hand landmark detection |
| `scikit-learn` | RandomForest classifier |
| `numpy` | Numerical operations |
| `matplotlib` / `seaborn` | Plots |
| `tqdm` | Progress bars |
| `pyttsx3` *(optional)* | Text-to-speech |

---

## ðŸ¤ Contributing

1. Fork the repo
2. Create your branch: `git checkout -b feature/your-feature`
3. Commit changes: `git commit -m "Add your feature"`
4. Push: `git push origin feature/your-feature`
5. Open a Pull Request

---

## ðŸ“„ License

MIT â€” free for personal and academic use.

---

## ðŸ‘¥ Team 112

Built as a final-year project exhibition. Refinements and extra features added for open-source release.
